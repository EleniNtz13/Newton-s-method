# Newton's Method for Two Variables (Python) ðŸ§®
This project implements **Newton's Method** for optimization of functions with two variables using Python.

It calculates:
- The **gradient** (first derivatives: âˆ‡f = [âˆ‚f/âˆ‚xâ€‹,âˆ‚f/âˆ‚y}â€‹
- The **Hessian matrix** (second derivatives: Î— = [âˆ‚^2f/âˆ‚x^2  âˆ‚^2f/â€‹â€‹âˆ‚xâˆ‚y  âˆ‚^2f/â€‹âˆ‚yâˆ‚x  âˆ‚^2f/â€‹âˆ‚y2â€‹â€‹] <img width="253" height="108" alt="image" src="https://github.com/user-attachments/assets/d5597dbd-b8c2-42f2-a642-31ce1eef4fe3" />

- The **determinant** of the Hessian (for checking invertibility)
- And automatically determines whether the result is a **local minimum**, **local maximum**, or **saddle point**

## How it works ðŸ’¡
Newton's method is an efficient optimization algorithm that uses both the gradient and Hessian of a function to find critical points.

The algorithm:
1. Takes a function f(x, y) as input
2. Computes the gradient and Hessian symbolically using **SymPy**
3. Iteratively updates the variables using \( x_{n+1} = x_n - H^{-1} * âˆ‡f(x_n) \) 
4. Stops when convergence is achieved or when the Hessian is not invertible

## How to run it ðŸš€
- Install sympy
- Install numpy

## Tool used ðŸ§ 
- ChatGPT AI Tool

## What I learned ðŸŽ¯
- Symbolic differentiation and Hessian computation with SymPy
- Matrix inversion and determinant checks with NumPy
- Convergence criteria for numerical optimization
